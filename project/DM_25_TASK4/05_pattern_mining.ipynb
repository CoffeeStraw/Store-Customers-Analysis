{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DATA MINING PROJECT: Analysis of a Supermarket’s Customers\n",
    "## 4) Pattern Mining\n",
    "### *Antonio Strippoli, Valerio Mariani*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsp import apriori\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set logging\n",
    "if os.path.exists('log.txt'):\n",
    "    os.remove('log.txt')\n",
    "logging.basicConfig(level=logging.INFO, filename=\"log.txt\", filemode=\"a+\", format=\"%(message)s\")\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "source": [
    "### Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    \"\"\"Read the dataset using Pandas, keeping only bought items.\"\"\"\n",
    "    df = pd.read_csv(\"../DM_25_TASK1/customer_supermarket_2.csv\", index_col=0, parse_dates=[\"PurchaseDate\"])\n",
    "    return df[df['Qta'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_baskets(df, threshold):\n",
    "    \"\"\"Keep only customers with more than `threshold` baskets.\"\"\"\n",
    "    customers = df.groupby('CustomerID').agg({'BasketID': 'nunique'})\n",
    "    customers = customers[customers >= threshold].dropna().index.values\n",
    "    return df[df['CustomerID'].isin(customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequentialize(df, return_times=False):\n",
    "    \"\"\"Convert a dataset into its sequential form. It can also return the time stamps of the baskets.\"\"\"\n",
    "    seq_data = []\n",
    "    times = []\n",
    "    for customer in df.groupby('CustomerID'):\n",
    "        customer = customer[1]\n",
    "        tmp = []\n",
    "        tmp2 = []\n",
    "        for basket in customer.groupby('BasketID'):\n",
    "            basket = basket[1]\n",
    "            purchases = list( basket['ProdID'].unique() )\n",
    "            time = basket['PurchaseDate'].max()\n",
    "            tmp.append(purchases)\n",
    "            tmp2.append(time)\n",
    "        seq_data.append(tmp)\n",
    "        times.append(tmp2)\n",
    "    if not return_times:\n",
    "        return seq_data\n",
    "    return seq_data, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(result_set, min_baskets, min_sup):\n",
    "    \"\"\"Save gsp results\"\"\"\n",
    "    with open(f'gsp_res/{min_baskets}mb_{int(min_sup*100)}ms.pickle', 'wb') as handle:\n",
    "        pickle.dump(result_set, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "### Apply GSP on sequential data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main cycle: apply GSP multiple times\n",
    "params = {\n",
    "    'min_sup': [0.4, 0.35, 0.3, 0.25, 0.2, 0.15],\n",
    "    'min_baskets': [20, 10, 5, 3, 2],\n",
    "}\n",
    "for min_sup in params['min_sup']:\n",
    "    for min_baskets in params['min_baskets']:\n",
    "        logging.info(f\"MIN_BASKETS: {min_baskets}, MIN_SUP: {min_sup}\")\n",
    "\n",
    "        # Read the dataset\n",
    "        df = read_dataset()\n",
    "        # Remove some baskets\n",
    "        df = remove_baskets(df, min_baskets)\n",
    "        # Convert into seq form\n",
    "        seq_data = sequentialize(df)\n",
    "        \n",
    "        # Apply GSP\n",
    "        t0 = time.time()\n",
    "        result_set = apriori(seq_data, min_sup, verbose=False)\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Compute n. of sequences with len > 2 and n. of sequences containing duplicates\n",
    "        cnt_len_2 = 0\n",
    "        cnt_duplicates = 0\n",
    "        for r in result_set:\n",
    "            r = r[0]\n",
    "            tmp = []\n",
    "            for l in r:\n",
    "                tmp.extend(l)\n",
    "            if len(tmp) >= 2:\n",
    "                cnt_len_2 += 1\n",
    "                if len(set(tmp)) < len(tmp):\n",
    "                    cnt_duplicates += 1\n",
    "\n",
    "        logging.info(\n",
    "            f\"TOTAL TIME:\\t{round(t1-t0, 2)} s\\n\"\\\n",
    "            f\"LEN RESULT SET:\\t{len(result_set)}\\n\"\\\n",
    "            f\"LEN SEQ > 2:\\t{cnt_len_2}\\nN. DUPLICATES:\\t{cnt_duplicates}\\n\"\n",
    "        )\n",
    "\n",
    "        # Save\n",
    "        save_to_pickle(result_set, min_baskets, min_sup)"
   ]
  },
  {
   "source": [
    "### Considerazioni\n",
    "\n",
    "Prendendo quelli che hanno fatto almeno 20 baskets, otteniamo una mole maggiore di risultati. Abbassando min_baskets, il supporto comincia a diventare sempre più basso in generale, il che lascia comunque presagire che nei clienti più occasionali non ci siano pattern evidenti.\n",
    "\n",
    "Il parametro min_baskets è importante perché altrimenti trovare dei pattern un po' più interessanti (che spazino tra basket diversi) è molto difficile (richiedono min_support bassi che alzano il costo computazionale). Alla fine abbiamo scelto 10 come giusto compromesso tra un numero di clienti abbastanza alto (il 10% di quelli di partenza) e sequenze variegate/lunghe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Analyze results and collect statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config (which result do we want to analyze)\n",
    "min_baskets = 10\n",
    "min_sup = 25\n",
    "\n",
    "# Read gsp results\n",
    "with open(f'gsp_res/{min_baskets}mb_{min_sup}ms.pickle', 'rb') as handle:\n",
    "    result_set = pickle.load(handle)\n",
    "# Sort by support\n",
    "result_set.sort(key=lambda x: x[1], reverse=True)\n",
    "# Prepare a copy\n",
    "result_set_original = result_set\n",
    "\n",
    "# Read and prepare the dataset\n",
    "df = read_dataset()\n",
    "df = remove_baskets(df, min_baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distribution of the lengths of sequences and n. of sequences containing duplicates\n",
    "cnt_len = {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "cnt_duplicates = 0\n",
    "for r in result_set:\n",
    "    r = r[0]\n",
    "    tmp = []\n",
    "    for l in r:\n",
    "        tmp.extend(l)\n",
    "    len_tmp = len(tmp)\n",
    "    cnt_len[len_tmp] += 1\n",
    "    if len(set(tmp)) < len_tmp:\n",
    "        cnt_duplicates += 1\n",
    "\n",
    "print(f\"Distribution of lengths: {cnt_len}\")\n",
    "print(f\"Sequences containing duplicates: {cnt_duplicates} / {len(result_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" PSEUDOCODICE\n",
    "per ogni cliente in clienti:\n",
    "    per ogni risultato in risultati:\n",
    "        per ogni carrello_1 in risultato: # carrello_res\n",
    "            per ogni carrello_2 in cliente: # carrello_customer\n",
    "                carrello_1 è contenuto in carrello_2?\n",
    "                    SI: scorri carrello_1 col prossimo carrello (fino a quando non finiscono, se finiscono allora questo cliente è ok)\n",
    "                    NO: scorri carrello_2 col prossimo carrello (fino a quando non finiscono, se finiscono allora questo cliente NON è ok)\n",
    "\"\"\"\n",
    "from copy import deepcopy\n",
    "\n",
    "result_set = deepcopy(result_set_original)\n",
    "\n",
    "# Prepare result_set\n",
    "for i in range(len(result_set)):\n",
    "    # Convert from tuple to list\n",
    "    result_set[i] = list(result_set[i])\n",
    "    tmp = []\n",
    "    # Create a nested list for future storage of mean qta of each item in the patterns\n",
    "    for basket in result_set[i][0]:\n",
    "        tmp2 = []\n",
    "        for item in basket:\n",
    "            tmp2.append(0)\n",
    "        tmp.append(tmp2)\n",
    "    result_set[i].append(tmp)\n",
    "\n",
    "# Find original transactions for each pattern and collect statistics\n",
    "out = []\n",
    "n_customers = len(df['CustomerID'].unique())\n",
    "for customer_i, customer in enumerate(df.groupby('CustomerID')):\n",
    "    # Progress\n",
    "    print(f\"{customer_i+1} / {n_customers}\")\n",
    "    # Extract baskets from the customer\n",
    "    baskets_customer = list(enumerate([x[1] for x in customer[1].groupby('BasketID')]))\n",
    "    \n",
    "    for result_i in range(len(result_set)):\n",
    "        res = result_set[result_i][0]\n",
    "\n",
    "        # Compare the baskets in the result against those of the customer\n",
    "        bc_i = 0\n",
    "        transactions = []\n",
    "        for basket_res in res:\n",
    "            for i, basket_customer in baskets_customer[bc_i:]:\n",
    "                entries = basket_customer[basket_customer['ProdID'].isin(basket_res)]\n",
    "                entries = entries.groupby('ProdID').aggregate({'Qta': 'sum'})\n",
    "                if len(entries) >= len(basket_res):\n",
    "                    bc_i = i + 1\n",
    "                    transactions.append(entries)\n",
    "                    break\n",
    "            else: # We iterated over all the baskets of the customer without finding a match for basket_res\n",
    "                break\n",
    "        else:\n",
    "            # Compute qta for each item in the pattern\n",
    "            for i, basket in enumerate(transactions):\n",
    "                for j in range(len(basket)):\n",
    "                    item = basket.iloc[j]\n",
    "                    result_set[result_i][2][i][j] += item['Qta']\n",
    "\n",
    "            out.append(transactions)\n",
    "\n",
    "# Compute mean of the qta previously found\n",
    "for res in result_set:\n",
    "    min_sup = res[1]\n",
    "    sup = min_sup * n_customers\n",
    "    for i in range(len(res[2])):\n",
    "        for j in range(len(res[2][i])):\n",
    "            res[2][i][j] = round(res[2][i][j] / sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ProductIDs to readable descriptions\n",
    "for r_i, result in enumerate(result_set):\n",
    "    tmp = []\n",
    "    for b_i, basket in enumerate(result_set[r_i][0]):\n",
    "        tmp2 = []\n",
    "        for p_i, p in enumerate(basket):\n",
    "            tmp2.append(df[df['ProdID'] == p]['ProdDescr'].iloc[0])\n",
    "        tmp.append(tmp2)\n",
    "    result_set[r_i][0] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_set"
   ]
  },
  {
   "source": [
    "### Considerazioni su RESULT_SET\n",
    "- Conta sequenze che hanno doppioni e quelle che non ce l'hanno (in percentuale?)\n",
    "- ogni evento è formato da un singolo elemento (non ci sono carrelli che hanno in comune più di un elemento, ma solo sequenze di carrelli con oggetti comuni)\n",
    "- Contare quante sono sequenze lunghe 1, 2, 3...\n",
    "\n",
    "### Considerazioni su transazioni e clienti a cui si riferiscono le sequenze\n",
    "- quantità media di oggetti presa per ogni oggetto di ogni sequenza\n",
    "- tempo passato tra una transazione e l'altra\n",
    "\n",
    "### Considerazioni\n",
    "due casi:\n",
    "- oggetti uguali: l'oggetto vende bene? Vengono ricomprati per essere venduti ancora\n",
    "- oggetti diversi: oggetto1 magari non ha venduto benissimo e se n'è comprato uno simile per provare a vendere quello, oppure ha comprato semplicemente un'altra variante."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Apply Time Constraints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Sanity check"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the dataset\n",
    "df = read_dataset()\n",
    "df = remove_baskets(df, 10)\n",
    "seq_data, times = sequentialize(df, return_times=True)\n",
    "\n",
    "print(times[0][0])\n",
    "print(times[1][0])\n",
    "print(type(times[0][0] - times[1][0]))\n",
    "\n",
    "# for s, t in zip(seq_data, times):\n",
    "#     if len(s) != len(t):\n",
    "#         assert False"
   ]
  },
  {
   "source": [
    "#### Re-apply the procedure introducing time constraints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = read_dataset()\n",
    "# Remove some baskets\n",
    "df = remove_baskets(df, 10)\n",
    "# Convert into seq form\n",
    "seq_data, times = sequentialize(df, return_times=True)\n",
    "\n",
    "# Apply GSP\n",
    "t0 = time.time()\n",
    "result_set = apriori(seq_data, 0.25)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "t0 = time.time()\n",
    "result_set2 = apriori(seq_data, 0.25, time_stamps=times, min_gap=datetime.timedelta(days=1))\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result_set))\n",
    "print(len(result_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}