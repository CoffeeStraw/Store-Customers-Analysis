{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DATA MINING PROJECT: Analysis of a Supermarket’s Customers\n",
    "## 4) Pattern Mining\n",
    "### *Antonio Strippoli, Valerio Mariani*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *  # Custom function for the analysis\n",
    "from gsp import apriori\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set logging\n",
    "if os.path.exists('log.txt'):\n",
    "    os.remove('log.txt')\n",
    "logging.basicConfig(level=logging.INFO, filename=\"log.txt\", filemode=\"a+\", format=\"%(message)s\")\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "source": [
    "Temporary cell here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Distribution of lengths: {1: 56, 2: 13, 3: 1, 4: 0, 5: 0}\nSequences containing duplicates: 9 / 70\n"
     ]
    }
   ],
   "source": [
    "# Config (which result do we want to analyze)\n",
    "min_baskets = 10\n",
    "min_sup = 0.25\n",
    "max_gap = datetime.timedelta(days=365)\n",
    "min_gap = datetime.timedelta(days=365)\n",
    "\n",
    "# Read the dataset\n",
    "df = read_dataset()\n",
    "# Remove some baskets\n",
    "df = remove_baskets(df, min_baskets)\n",
    "# Convert into seq form\n",
    "seq_data, time_stamps = sequentialize(df, return_times=True)\n",
    "\n",
    "# Apply GSP\n",
    "result_set = apriori(seq_data, min_sup, time_stamps, max_span=None, min_gap=None, max_gap=max_gap)\n",
    "print_distribution(result_set)\n",
    "\n",
    "# Distribution of lengths: {1: 56, 2: 1, 3: 0, 4: 0, 5: 0}"
   ]
  },
  {
   "source": [
    "### Apply GSP on sequential data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main cycle: apply GSP multiple times\n",
    "params = {\n",
    "    'min_sup': [0.4, 0.35, 0.3, 0.25, 0.2, 0.15],\n",
    "    'min_baskets': [20, 10, 5, 3, 2],\n",
    "}\n",
    "for min_sup in params['min_sup']:\n",
    "    for min_baskets in params['min_baskets']:\n",
    "        logging.info(f\"MIN_BASKETS: {min_baskets}, MIN_SUP: {min_sup}\")\n",
    "\n",
    "        # Read the dataset\n",
    "        df = read_dataset()\n",
    "        # Remove some baskets\n",
    "        df = remove_baskets(df, min_baskets)\n",
    "        # Convert into seq form\n",
    "        seq_data = sequentialize(df)\n",
    "        \n",
    "        # Apply GSP\n",
    "        t0 = time.time()\n",
    "        result_set = apriori(seq_data, min_sup, verbose=False)\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Compute n. of sequences with len > 2 and n. of sequences containing duplicates\n",
    "        cnt_len_2 = 0\n",
    "        cnt_duplicates = 0\n",
    "        for r in result_set:\n",
    "            r = r[0]\n",
    "            tmp = []\n",
    "            for l in r:\n",
    "                tmp.extend(l)\n",
    "            if len(tmp) >= 2:\n",
    "                cnt_len_2 += 1\n",
    "                if len(set(tmp)) < len(tmp):\n",
    "                    cnt_duplicates += 1\n",
    "\n",
    "        logging.info(\n",
    "            f\"TOTAL TIME:\\t{round(t1-t0, 2)} s\\n\"\\\n",
    "            f\"LEN RESULT SET:\\t{len(result_set)}\\n\"\\\n",
    "            f\"LEN SEQ > 2:\\t{cnt_len_2}\\nN. DUPLICATES:\\t{cnt_duplicates}\\n\"\n",
    "        )\n",
    "\n",
    "        # Save\n",
    "        save_to_pickle(result_set, min_baskets, min_sup)"
   ]
  },
  {
   "source": [
    "### Considerazioni\n",
    "\n",
    "Prendendo quelli che hanno fatto almeno 20 baskets, otteniamo una mole maggiore di risultati. Abbassando min_baskets, il supporto comincia a diventare sempre più basso in generale, il che lascia comunque presagire che nei clienti più occasionali non ci siano pattern evidenti.\n",
    "\n",
    "Il parametro min_baskets è importante perché altrimenti trovare dei pattern un po' più interessanti (che spazino tra basket diversi) è molto difficile (richiedono min_support bassi che alzano il costo computazionale). Alla fine abbiamo scelto 10 come giusto compromesso tra un numero di clienti abbastanza alto (il 10% di quelli di partenza) e sequenze variegate/lunghe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Analyze results and collect statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config (which result do we want to analyze)\n",
    "min_baskets = 10\n",
    "min_sup = 25\n",
    "\n",
    "# Read result\n",
    "result_set = read_result(min_baskets, min_sup)\n",
    "result_set = convert_tuples_to_list(result_set)\n",
    "print_distribution(result_set)\n",
    "\n",
    "# Read and prepare the dataset\n",
    "df = read_dataset()\n",
    "df = remove_baskets(df, min_baskets)\n",
    "\n",
    "# Compute mean qta values\n",
    "result_set = compute_patterns_mean_qta(result_set, df)\n",
    "\n",
    "# Convert ProdID to ProdDescr\n",
    "result_set = prodID_to_prodDescr(result_set, df)\n",
    "\n",
    "result_set"
   ]
  },
  {
   "source": [
    "### Considerazioni su RESULT_SET\n",
    "- Conta sequenze che hanno doppioni e quelle che non ce l'hanno (in percentuale?)\n",
    "- ogni evento è formato da un singolo elemento (non ci sono carrelli che hanno in comune più di un elemento, ma solo sequenze di carrelli con oggetti comuni)\n",
    "- Contare quante sono sequenze lunghe 1, 2, 3...\n",
    "\n",
    "### Considerazioni su transazioni e clienti a cui si riferiscono le sequenze\n",
    "- quantità media di oggetti presa per ogni oggetto di ogni sequenza\n",
    "- tempo passato tra una transazione e l'altra\n",
    "\n",
    "### Considerazioni\n",
    "due casi:\n",
    "- oggetti uguali: l'oggetto vende bene? Vengono ricomprati per essere venduti ancora\n",
    "- oggetti diversi: oggetto1 magari non ha venduto benissimo e se n'è comprato uno simile per provare a vendere quello, oppure ha comprato semplicemente un'altra variante."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Apply Time Constraints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'min_sup': [0.4, 0.35, 0.3, 0.25, 0.2, 0.15],\n",
    "    'min_baskets': [20, 10],\n",
    "    'max_gap': [datetime.timedelta(days=1), datetime.timedelta(days=2), datetime.timedelta(days=3), datetime.timedelta(weeks=1), datetime.timedelta(weeks=2), datetime.timedelta(weeks=3), datetime.timedelta(weeks=4), datetime.timedelta(weeks=8), datetime.timedelta(weeks=12)],\n",
    "    'max_span': [datetime.timedelta(weeks=4), datetime.timedelta(weeks=8), datetime.timedelta(weeks=12), datetime.timedelta(weeks=48)]\n",
    "}\n",
    "for min_sup in params['min_sup']:\n",
    "    for min_baskets in params['min_baskets']:\n",
    "        for max_gap in params['max_gap']:\n",
    "            for max_span in params['max_span']:\n",
    "                logging.info(f\"MIN_BASKETS: {min_baskets}, MIN_SUP: {min_sup}, MAX_GAP: {max_gap}, MAX_SPAN: {max_span}\")\n",
    "\n",
    "                # Read the dataset\n",
    "                df = read_dataset()\n",
    "                # Remove some baskets\n",
    "                df = remove_baskets(df, min_baskets)\n",
    "                # Convert into seq form\n",
    "                seq_data, time_stamps = sequentialize(df, return_times=True)\n",
    "                \n",
    "                # Apply GSP\n",
    "                t0 = time.time()\n",
    "                result_set = apriori(seq_data, min_sup, time_stamps, max_span=max_span, min_gap=None, max_gap=max_gap)\n",
    "                t1 = time.time()\n",
    "\n",
    "                # Compute n. of sequences with len > 2 and n. of sequences containing duplicates\n",
    "                cnt_len_2 = 0\n",
    "                cnt_duplicates = 0\n",
    "                for r in result_set:\n",
    "                    r = r[0]\n",
    "                    tmp = []\n",
    "                    for l in r:\n",
    "                        tmp.extend(l)\n",
    "                    if len(tmp) >= 2:\n",
    "                        cnt_len_2 += 1\n",
    "                        if len(set(tmp)) < len(tmp):\n",
    "                            cnt_duplicates += 1\n",
    "\n",
    "                logging.info(\n",
    "                    f\"TOTAL TIME:\\t{round(t1-t0, 2)} s\\n\"\\\n",
    "                    f\"LEN RESULT SET:\\t{len(result_set)}\\n\"\\\n",
    "                    f\"LEN SEQ > 2:\\t{cnt_len_2}\\nN. DUPLICATES:\\t{cnt_duplicates}\\n\"\n",
    "                )\n",
    "\n",
    "                # Save\n",
    "                save_to_pickle(result_set, min_baskets, min_sup, max_gap.days, max_span.days)"
   ]
  },
  {
   "source": [
    "#### Re-apply the procedure introducing time constraints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config (which result do we want to analyze)\n",
    "min_baskets = 10\n",
    "min_sup = 0.25\n",
    "max_gap = 24\n",
    "max_span = None\n",
    "\n",
    "cnt = 0\n",
    "while True:\n",
    "    print(\"TESTING max_gap =\", max_gap)\n",
    "    # Read the dataset\n",
    "    df = read_dataset()\n",
    "    # Remove some baskets\n",
    "    df = remove_baskets(df, min_baskets)\n",
    "    # Convert into seq form\n",
    "    seq_data, time_stamps = sequentialize(df, return_times=True)\n",
    "\n",
    "    # Apply GSP\n",
    "    result_set = apriori(seq_data, min_sup, time_stamps, max_span=None, min_gap=None, max_gap=datetime.timedelta(weeks=max_gap))\n",
    "    max_gap += 4\n",
    "\n",
    "    if not cnt:\n",
    "        cnt = len(result_set)\n",
    "        print('LEN RESULT SET:', cnt)\n",
    "        continue\n",
    "\n",
    "    if cnt > len(result_set):\n",
    "        cnt = len(result_set)\n",
    "        print('LEN RESULT SET INCREMENTED:', cnt)\n",
    "        if cnt >= 73:\n",
    "            assert False\n",
    "\n",
    "# Sort by support\n",
    "result_set.sort(key=lambda x: x[1], reverse=True)\n",
    "# Prepare a copy\n",
    "result_set_original = result_set\n",
    "\n",
    "# Read and prepare the dataset\n",
    "df = read_dataset()\n",
    "df = remove_baskets(df, min_baskets)"
   ]
  }
 ]
}